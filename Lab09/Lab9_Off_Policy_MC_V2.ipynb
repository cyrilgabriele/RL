{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrilgabriele/RL/blob/main/Lab09/Lab9_Off_Policy_MC_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monte Carlo with Q-Function (Action-Value Fucntion Q(s,a)) as State Value Function**\n"
      ],
      "metadata": {
        "id": "ZDvY3YLW59Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, namedtuple\n",
        "from enum import Enum\n",
        "from typing import Tuple, List\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import copy\n",
        "import time\n",
        "import math\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "F6lBthee6oM5"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "nGdFeOFo507u"
      },
      "outputs": [],
      "source": [
        "Point = namedtuple('Point', ['x', 'y'])\n",
        "class Direction(Enum):\n",
        "  NORTH = \"⬆\"\n",
        "  EAST = \"⮕\"\n",
        "  SOUTH = \"⬇\"\n",
        "  WEST = \"⬅\"\n",
        "\n",
        "  @classmethod\n",
        "  def values(self):\n",
        "    return [v for v in self]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our environment => like self.env = gym.make('CartPole-v0') from Lab02\n",
        "class SimpleGridWorld(object):\n",
        "\n",
        "  def __init__(self, width: int = 5, height: int = 5, debug: bool = False):\n",
        "    print(\"This is our environment\")\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.debug = debug\n",
        "    self.action_space = [d for d in Direction]\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.cur_pos = Point(x=0, y=(self.height - 1))\n",
        "    self.goal = Point(x=(self.width - 1), y=0)\n",
        "    # If debug, print state\n",
        "    if self.debug:\n",
        "      print(self)\n",
        "    return self.cur_pos, 0, False\n",
        "\n",
        "  def step(self, action: Direction):\n",
        "    # Depending on the action, mutate the environment state\n",
        "    if action == Direction.NORTH:\n",
        "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y + 1)\n",
        "    elif action == Direction.EAST:\n",
        "      self.cur_pos = Point(self.cur_pos.x + 1, self.cur_pos.y)\n",
        "    elif action == Direction.SOUTH:\n",
        "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y - 1)\n",
        "    elif action == Direction.WEST:\n",
        "      self.cur_pos = Point(self.cur_pos.x - 1, self.cur_pos.y)\n",
        "    # Check if out of bounds\n",
        "    if self.cur_pos.x >= self.width:\n",
        "      self.cur_pos = Point(self.width - 1, self.cur_pos.y)\n",
        "    if self.cur_pos.y >= self.height:\n",
        "      self.cur_pos = Point(self.cur_pos.x, self.height - 1)\n",
        "    if self.cur_pos.x < 0:\n",
        "      self.cur_pos = Point(0, self.cur_pos.y)\n",
        "    if self.cur_pos.y < 0:\n",
        "      self.cur_pos = Point(self.cur_pos.x, 0)\n",
        "\n",
        "    # If at goal, terminate\n",
        "    is_terminal = self.cur_pos == self.goal\n",
        "\n",
        "    # Constant -1 reward to promote speed-to-goal\n",
        "\n",
        "    reward = -1\n",
        "\n",
        "    # If debug, print state\n",
        "    if self.debug:\n",
        "      print(self)\n",
        "\n",
        "    return self.cur_pos, reward, is_terminal\n",
        "\n",
        "  def peek(self, action: Direction):\n",
        "  # get next position without mutating the environment\n",
        "    if action == Direction.NORTH:\n",
        "      new_pos = Point(self.cur_pos.x, self.cur_pos.y + 1)\n",
        "    elif action == Direction.EAST:\n",
        "       new_pos = Point(self.cur_pos.x + 1, self.cur_pos.y)\n",
        "    elif action == Direction.SOUTH:\n",
        "       new_pos = Point(self.cur_pos.x, self.cur_pos.y - 1)\n",
        "    elif action == Direction.WEST:\n",
        "      new_pos = Point(self.cur_pos.x - 1, self.cur_pos.y)\n",
        "    # Check if out of bounds\n",
        "    if new_pos.x >= self.width:\n",
        "      new_pos = Point(self.width - 1, self.cur_pos.y)\n",
        "    if new_pos.y >= self.height:\n",
        "      new_pos = Point(self.cur_pos.x, self.height - 1)\n",
        "    if new_pos.x < 0:\n",
        "      new_pos = Point(0, self.cur_pos.y)\n",
        "    if new_pos.y < 0:\n",
        "      new_pos = Point(self.cur_pos.x, 0)\n",
        "    return new_pos\n",
        "\n",
        "  def get_valid_actions(self):\n",
        "    # get only the actions that change the current position\n",
        "    valid_actions = []\n",
        "    for action in self.action_space:\n",
        "      if self.peek(action) != self.cur_pos:\n",
        "        valid_actions.append(action)\n",
        "    return valid_actions\n",
        "\n",
        "  def __repr__(self):\n",
        "    res = \"\"\n",
        "    for y in reversed(range(self.height)):\n",
        "      for x in range(self.width):\n",
        "        if self.goal.x == x and self.goal.y == y:\n",
        "          if self.cur_pos.x == x and self.cur_pos.y == y:\n",
        "            res += \"@\"\n",
        "          else:\n",
        "            res += \"o\"\n",
        "          continue\n",
        "        if self.cur_pos.x == x and self.cur_pos.y == y:\n",
        "          res += \"x\"\n",
        "        else:\n",
        "          res += \"_\"\n",
        "      res += \"\\n\"\n",
        "    return res"
      ],
      "metadata": {
        "id": "POTVXWVc6lGt"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloGeneration(object):\n",
        "  def __init__(self, env: object, max_steps: int = 1000, debug: bool = False, decay=50, min_epsilon=0.1):\n",
        "    self.env = env\n",
        "    self.max_steps = max_steps\n",
        "    self.debug = debug\n",
        "    # self.steps = np.zeros(self.num_episodes)\n",
        "\n",
        "    # Create a Q-table dictionary and initialize it random\n",
        "    self.Q_table = defaultdict(lambda: defaultdict(float))\n",
        "    for x in range(self.env.width):\n",
        "      for y in range(self.env.height):\n",
        "        state = Point(x, y)\n",
        "        for action in self.env.action_space:\n",
        "            self.Q_table[state][action] = np.random.random()\n",
        "\n",
        "  def policy_b():\n",
        "    action = random.choice(self.env.action_space) # Random action => soft policy\n",
        "    return action\n",
        "\n",
        "  def run(self, n_run) -> List:\n",
        "    buffer = []\n",
        "    n_steps = 0 # Keep track of the number of steps so I can bail out if it takes too long\n",
        "    state, _, _ = self.env.reset() # Reset environment back to start\n",
        "    terminal = False\n",
        "    while not terminal: # Run until terminal state\n",
        "\n",
        "        action = self.policy_b\n",
        "\n",
        "        next_state, reward, terminal = self.env.step(action) # Take action in environment\n",
        "        buffer.append((state, action, reward)) # Store the result\n",
        "        state = next_state # Ready for the next step\n",
        "        n_steps += 1\n",
        "        if n_steps >= self.max_steps:\n",
        "          if self.debug:\n",
        "            print(\"Terminated early due to large number of steps\")\n",
        "          terminal = True # Bail out if we've been working for too long\n",
        "    return buffer"
      ],
      "metadata": {
        "id": "5qVUZWM37U2P"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class is our agent\n",
        "class MonteCarloExperiment(object):\n",
        "  def __init__(self, generator: MonteCarloGeneration, num_episodes=1000, min_lr=0.1, min_epsilon=0.1, discount=1.0, decay=10):\n",
        "    self.generator = generator\n",
        "    self.num_episodes = num_episodes\n",
        "    self.min_lr = min_lr\n",
        "    self.min_epsilon = min_epsilon\n",
        "    # Discount factor gamma\n",
        "    self.discount = discount\n",
        "    self.decay = decay\n",
        "    self.env = env\n",
        "    self.steps = np.zeros(self.num_episodes)\n",
        "\n",
        "# Create a Q-table dictionary and initialize it random\n",
        "    self.Q_table = defaultdict(lambda: defaultdict(float))\n",
        "    for x in range(self.env.width):\n",
        "      for y in range(self.env.height):\n",
        "        state = Point(x, y)\n",
        "        for action in self.env.action_space:\n",
        "            self.Q_table[state][action] = np.random.rand()\n",
        "\n",
        "# Create a C-table dictionary and initialize it with all 0.0\n",
        "    self.C_table = defaultdict(lambda: defaultdict(float))\n",
        "    for x in range(self.env.width):\n",
        "      for y in range(self.env.height):\n",
        "        state = Point(x, y)\n",
        "        for action in self.env.action_space:\n",
        "            self.C_table[state][action] = 0.0\n",
        "\n",
        "\n",
        "  '''\n",
        "  def choose_action(self, state):\n",
        "    if (np.random.random() < self.epsilon):\n",
        "        return random.choice(self.env.action_space)\n",
        "    else:\n",
        "        return max(self.env.action_space, key=lambda a: self.Q_table[state][a])\n",
        "  '''\n",
        "\n",
        "  def update_q(self, state, action, W, G):\n",
        "    \"\"\"\n",
        "    Updates Q-table using the rule as described by Sutton and Barto in\n",
        "    Reinforcement Learning.\n",
        "    \"\"\"\n",
        "\n",
        "    self.Q_table[state][action] += (W/self.C_table[state][action]) * (G - self.Q_table[state][action])\n",
        "\n",
        "  def update_c(self, state, action, W):\n",
        "    \"\"\"\n",
        "    Updates C using the rule as described by Sutton and Barto Section 5.7.\n",
        "    \"\"\"\n",
        "\n",
        "    self.C_table[state][action] += W\n",
        "\n",
        "\n",
        "  def get_epsilon(self, t):\n",
        "    \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
        "    # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
        "    return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "  def get_learning_rate(self, t):\n",
        "    \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
        "    # Learning rate also declines as we add more episodes\n",
        "    return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "  def run_episode(self, n_run) -> None:\n",
        "    G = 0 # Return G\n",
        "    W = 1\n",
        "    trajectory = self.generator.run(n_run) # Generate a trajectory\n",
        "    #print(\"this is trjectory: \", trajectory)\n",
        "    for i, t in enumerate(reversed(trajectory)): # Starting from the terminal state\n",
        "      state, action, reward = t\n",
        "      G += self.discount * G + reward\n",
        "\n",
        "      # TODO fix the states here => maybe need to be returned from run()\n",
        "      # Update C\n",
        "      self.update_c(state, action, W)\n",
        "      # Update Q\n",
        "      self.update_q(state, action, W, G)\n",
        "\n",
        "      policy_pi = max(self.Q_table[state].values())\n",
        "      if action is not policy_pi:\n",
        "        break\n",
        "      else:\n",
        "        W += (1/random.choice(self.env.action_space))"
      ],
      "metadata": {
        "id": "h1Ow4xMd8Egr"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_best_value_2d(agent):\n",
        "  res = \"\"\n",
        "  for y in reversed(range(agent.env.height)):\n",
        "    for x in range(agent.env.width):\n",
        "      state = Point(x, y)\n",
        "      if agent.env.goal.x == x and agent.env.goal.y == y:\n",
        "        res += \"@\"\n",
        "      else:\n",
        "        # Find the action that has the highest value\n",
        "        best_action = max(agent.env.action_space, key=lambda a: agent.Q_table[state][a])\n",
        "        q_value = agent.Q_table[state][best_action]\n",
        "        res += f'{best_action.value} ({q_value:.2f})'\n",
        "      res += \" | \"\n",
        "    res += \"\\n\"\n",
        "  return res"
      ],
      "metadata": {
        "id": "SUh6Lj1APqcP"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SimpleGridWorld() # Instantiate the environment\n",
        "generator = MonteCarloGeneration(env=env) # Instantiate the trajectory generator\n",
        "agent = MonteCarloExperiment(generator=generator)\n",
        "for i in range(1000):\n",
        "  clear_output(wait=True)\n",
        "  agent.run_episode(i)\n",
        "  print(f\"Iteration: {i}\")\n",
        "  print(next_best_value_2d(agent))\n",
        "  # time.sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxlsKBYwOdOr",
        "outputId": "54478fab-5d58-447c-e608-600ef7b5d3cc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 999\n",
            "⬇ (0.30) | ⮕ (0.89) | ⬆ (0.91) | ⮕ (0.89) | ⮕ (0.89) | \n",
            "⬇ (0.36) | ⮕ (0.97) | ⮕ (0.43) | ⮕ (0.62) | ⬆ (0.27) | \n",
            "⮕ (0.85) | ⬆ (0.60) | ⬆ (0.92) | ⬆ (0.96) | ⬅ (0.67) | \n",
            "⬇ (0.69) | ⮕ (0.94) | ⮕ (0.62) | ⬅ (0.91) | ⬇ (0.86) | \n",
            "⬅ (0.44) | ⬅ (0.65) | ⬇ (0.94) | ⬅ (0.91) | @ | \n",
            "\n"
          ]
        }
      ]
    }
  ]
}