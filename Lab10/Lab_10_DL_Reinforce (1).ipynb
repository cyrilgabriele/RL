{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cbhF6-H5x_b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f8e55f-bf2f-491e-d0ae-76dbf090d3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1))\n",
            "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-qegs2mw8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-qegs2mw8\n",
            "  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/simoninithomas/gym-games (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n",
            "  Cloning https://github.com/simoninithomas/gym-games to /tmp/pip-req-build-a1vw3ar6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games /tmp/pip-req-build-a1vw3ar6\n",
            "  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.19.4)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (0.4.9)\n",
            "Requirement already satisfied: pyyaml==6.0 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 5)) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: gym>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.25.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: pygame>=1.9.6 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.0.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDr7ZGY-x_b9"
      },
      "source": [
        "# Q-Learning on the CartPole Environment\n",
        "\n",
        "This is an altered version of Jose Nieves Flores Maynez' notebook.\n",
        "\n",
        "This tutorial shows how to use Q-Learning to train an RL agent on the CartPole-v0 task from the [OpenAI Gym](https://gym.openai.com/).\n",
        "\n",
        "![cartpole](https://github.com/pytorch/tutorials/blob/main/_static/img/cartpole.gif?raw=true)\n",
        "\n",
        "The Cartpole environment is a common simple example that is used often for simple RL examples.\n",
        "\n",
        "In this environment, the task is to balance the pole that is attached to the cart, by moving the cart to either side.\n",
        "The reward gets incremented for each step (for up to 500 steps => cartpole-v1 max steps = 500 NOT 200!! ref:https://www.gymlibrary.dev/environments/classic_control/cart_pole/) where the pole is not exceeding a set angle and the cart is not touching the sides of the line.\n",
        "The environment provides four parameters that represent the state of the environment:\n",
        "Position and velocity of the cart and angle and angular velocity of the pole (see [the documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space)).\n",
        "We will solve this by applying Q-Learning to our RL agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Packages\n",
        "\n",
        "\n",
        "First, let's import needed packages."
      ],
      "metadata": {
        "id": "QCWbuk66234H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9ccdmXWUx_b_"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "# Hugging Face Hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if we have a GPU"
      ],
      "metadata": {
        "id": "XYndtRLbU11C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "hrJ2gg3LU1HF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4980003-19e5-4b54-d6fd-e1fb975dd3c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psxbGvWJx_cB"
      },
      "source": [
        "## Implementation\n",
        "Since this algorithm relies on updating a function for each existing pair of state and action, environments that have a high state-space become problematic. This is because we can approximate better the actual value of a state-action pair as we visit it more often. However, if we have many states or many actions to take, we distribute our visits among more pairs and it takes much longer to converge to the actual true values. The CartPole environment gives us the position of the cart, its velocity, the angle of the pole and the velocity at the tip of the pole as descriptors of the state. However, all of these are continuous variables. To be able to solve this problem, we need to discretize these states since otherwise, it would take forever to get values for each of the possible combinations of each state, despite them being bounded. The solution is to group several values of each of the variables into the same “bucket” and treat them as similar states. The agent implemented for this problem uses 3, 3, 6, and 6 buckets respectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "DLHhz65OabE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f39c1f8-09fa-4dff-ec4a-3b7e5a43c324"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforceAgent(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        \"\"\"\n",
        "        Initialize agent.\n",
        "\n",
        "        :param s_size: The size of the state vector\n",
        "        :param a_size: The size of the action vector\n",
        "        :param h_size: The size of the hidden state vector\n",
        "        \"\"\"\n",
        "        super(ReinforceAgent, self).__init__()\n",
        "        # TODO: Create two fully connected layers\n",
        "        self.inputLayer = nn.Linear(s_size, h_size) # Input Layer\n",
        "        self.fc1 = nn.ReLU()\n",
        "        self.hidden1 = nn.Linear(h_size, h_size) # Hidden Layer 1\n",
        "        self.fc2 = nn.ReLU()\n",
        "        self.hidden2 = nn.Linear(h_size, a_size) # Hidden Layer 2\n",
        "        self.fcOutput = nn.Softmax()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define the forward pass\n",
        "        # state goes to fc1 then we apply ReLU activation function\n",
        "        x = self.fc1(self.inputLayer(x))\n",
        "        # fc1 outputs goes to fc2\n",
        "        x = self.fc2(self.hidden1(x))\n",
        "        # We output the softmax\n",
        "        x = self.fcOutput(self.hidden2(x))\n",
        "        return x\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Given a state, take action\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        # print(f\"here are the probs: \\n{probs}\")\n",
        "        m = Categorical(probs)\n",
        "        # print(f\"strange m: {m}\")\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)\n",
        "\n",
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # Help us to calculate the score during the training\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "\n",
        "    for i_episode in trange(1, n_training_episodes + 1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "\n",
        "        returns = deque(maxlen=max_t)\n",
        "        n_steps = len(rewards)\n",
        "        # Compute the discounted returns at each timestep,\n",
        "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
        "        #\n",
        "        # In O(N) time, where N is the number of time steps\n",
        "        # (this definition of the discounted return G_t follows the definition of this quantity\n",
        "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
        "        # G_t = r_(t+1) + r_(t+2) + ...\n",
        "\n",
        "        # Given this formulation, the returns at each timestep t can be computed\n",
        "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
        "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
        "        # G_(t-1) = r_t + gamma* G_t\n",
        "        # (this follows a dynamic programming approach, with which we memorize solutions in order\n",
        "        # to avoid computing them multiple times)\n",
        "\n",
        "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
        "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
        "\n",
        "        ## Given the above, we calculate the returns at timestep t as:\n",
        "        #               gamma[t] * return[t] + reward[t]\n",
        "        #\n",
        "        ## We compute this starting from the last timestep to the first, in order\n",
        "        ## to employ the formula presented above and avoid redundant computations that would be needed\n",
        "        ## if we were to do it from first to last.\n",
        "\n",
        "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
        "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
        "        ## a normal python list would instead require O(N) to do this.\n",
        "        for t in range(n_steps)[::-1]:\n",
        "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
        "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
        "\n",
        "        ## standardization of the returns is employed to make training more stable\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        ## eps is the smallest representable float, which is\n",
        "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        policy_loss = []\n",
        "        # Here the loss function is calculated => same equation as in lecture\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Here the backpropagation is calculated with respect to the above defined loss function\n",
        "        policy_loss.backward()\n",
        "        # Here the NN updates the parameters theta\n",
        "        optimizer.step()\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            tqdm.write(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 1000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"state_space\": env.observation_space.shape[0],\n",
        "    \"action_space\": env.action_space.n,\n",
        "}\n",
        "\n",
        "\n",
        "def load_reinforce_agent():\n",
        "    cartpole_policy = ReinforceAgent(\n",
        "        cartpole_hyperparameters[\"state_space\"],\n",
        "        cartpole_hyperparameters[\"action_space\"],\n",
        "        cartpole_hyperparameters[\"h_size\"],\n",
        "    ).to(device)\n",
        "    cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])\n",
        "    scores = reinforce(\n",
        "        cartpole_policy,\n",
        "        cartpole_optimizer,\n",
        "        cartpole_hyperparameters[\"n_training_episodes\"],\n",
        "        cartpole_hyperparameters[\"max_t\"],\n",
        "        cartpole_hyperparameters[\"gamma\"],\n",
        "        100,\n",
        "    )\n",
        "    return cartpole_policy\n",
        "\n",
        "agent = load_reinforce_agent()"
      ],
      "metadata": {
        "id": "tqqVqJYZwjQr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231,
          "referenced_widgets": [
            "618366c453d14dceb58b06cb6276361a",
            "8767efcfbdb84c5f94516d7a798c6ef7",
            "561f5e9fb80e4838a4282984ee135b7b",
            "81ed69ccad2d4b9290b83778f01d489f",
            "e643610a04e74fa291ca88fbc48f78af",
            "94f7ef8adc004a22ab2e09f6a535291d",
            "c7d01a6e1b744f2bb555ac39b734e008",
            "bbbe2628cfbc41b7994fd33a63c485a0",
            "62ae2d4fac8c43908203809bb82a01a4",
            "511a87e6f38443b6b838cdb16f9d8a82",
            "6e28ca646aa14f6a98caef6379d0383a"
          ]
        },
        "outputId": "2e003035-cefb-4864-b705-a343def99bb7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "618366c453d14dceb58b06cb6276361a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 35.44\n",
            "Episode 200\tAverage Score: 258.99\n",
            "Episode 300\tAverage Score: 169.97\n",
            "Episode 400\tAverage Score: 500.00\n",
            "Episode 500\tAverage Score: 500.00\n",
            "Episode 600\tAverage Score: 398.98\n",
            "Episode 700\tAverage Score: 410.24\n",
            "Episode 800\tAverage Score: 413.66\n",
            "Episode 900\tAverage Score: 337.45\n",
            "Episode 1000\tAverage Score: 144.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning(stepsPerEpisode: int, n_eval_episodes: int) -> None:\n",
        "  plt.scatter(n_eval_episodes, stepsPerEpisode)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "m_IfY04fzloR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "    \"\"\"\n",
        "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "    :param env: The evaluation environment\n",
        "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "    :param policy: The Reinforce agent\n",
        "    \"\"\"\n",
        "    # For plotting the steps per episode:\n",
        "    stepsPerEpisode = []\n",
        "    totalSteps = 0\n",
        "\n",
        "    episode_rewards = []\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        done = False\n",
        "        total_rewards_ep = 0\n",
        "\n",
        "        totalSteps = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action, _ = policy.act(state)\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            total_rewards_ep += reward\n",
        "            totalSteps += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            state = new_state\n",
        "        stepsPerEpisode.append(totalSteps)\n",
        "        episode_rewards.append(total_rewards_ep)\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "    plot_learning(stepsPerEpisode, range(n_eval_episodes))\n",
        "\n",
        "    return mean_reward, std_reward\n",
        "\n",
        "evaluate_agent(\n",
        "    env, cartpole_hyperparameters[\"max_t\"], cartpole_hyperparameters[\"n_evaluation_episodes\"], agent\n",
        ")"
      ],
      "metadata": {
        "id": "ssL5Q92eXEko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "bb226866-8b89-4e6c-cd89-20b91f2b28b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f9cdccbc2a3a>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m evaluate_agent(\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartpole_hyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_t\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartpole_hyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_evaluation_episodes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "As we can see the model fully converged to the maximum of 500 (for the reward). Furthermore we have a standart deviation of 0.0!"
      ],
      "metadata": {
        "id": "48Lo6g18xDA3"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "618366c453d14dceb58b06cb6276361a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8767efcfbdb84c5f94516d7a798c6ef7",
              "IPY_MODEL_561f5e9fb80e4838a4282984ee135b7b",
              "IPY_MODEL_81ed69ccad2d4b9290b83778f01d489f"
            ],
            "layout": "IPY_MODEL_e643610a04e74fa291ca88fbc48f78af"
          }
        },
        "8767efcfbdb84c5f94516d7a798c6ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f7ef8adc004a22ab2e09f6a535291d",
            "placeholder": "​",
            "style": "IPY_MODEL_c7d01a6e1b744f2bb555ac39b734e008",
            "value": "100%"
          }
        },
        "561f5e9fb80e4838a4282984ee135b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbbe2628cfbc41b7994fd33a63c485a0",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62ae2d4fac8c43908203809bb82a01a4",
            "value": 1000
          }
        },
        "81ed69ccad2d4b9290b83778f01d489f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_511a87e6f38443b6b838cdb16f9d8a82",
            "placeholder": "​",
            "style": "IPY_MODEL_6e28ca646aa14f6a98caef6379d0383a",
            "value": " 1000/1000 [07:02&lt;00:00,  6.11it/s]"
          }
        },
        "e643610a04e74fa291ca88fbc48f78af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f7ef8adc004a22ab2e09f6a535291d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7d01a6e1b744f2bb555ac39b734e008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbbe2628cfbc41b7994fd33a63c485a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ae2d4fac8c43908203809bb82a01a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "511a87e6f38443b6b838cdb16f9d8a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e28ca646aa14f6a98caef6379d0383a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}