{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrilgabriele/AI1/blob/main/Lab3_Task1%2B2_Cyril_Ardi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monte Carlo with incremental (value iteration) State Value Function**\n",
        "\n",
        "Monte Carlo with State Value Function: For the first task, you are required to take the Monte Carlo algorithm from the first week's lab (Grid World) and modify it to work with a state-value function instead of a random choice. This means that instead of randomly sampling from all possible actions, you sample according to the estimated state-value function. You should also incorporate a strategy to balance exploration and exploitation using ε-Greedy.\n",
        "\n",
        "Incremental Monte Carlo: In the second task, extend the Monte Carlo algorithm from the first task to an incremental Monte Carlo method. Incremental Monte Carlo updates the value estimates incrementally, reducing the need to wait until the end of an episode to update values. This should lead to more efficient learning.\n",
        "Contributors: Cyril Gabriele (gabricyr), Ardi Jasari (jasarard)"
      ],
      "metadata": {
        "id": "ZDvY3YLW59Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, namedtuple\n",
        "from enum import Enum\n",
        "from typing import Tuple, List\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import copy\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "F6lBthee6oM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGdFeOFo507u"
      },
      "outputs": [],
      "source": [
        "Point = namedtuple('Point', ['x', 'y'])\n",
        "class Direction(Enum):\n",
        "  NORTH = \"⬆\"\n",
        "  EAST = \"⮕\"\n",
        "  SOUTH = \"⬇\"\n",
        "  WEST = \"⬅\"\n",
        "\n",
        "  @classmethod\n",
        "  def values(self):\n",
        "    return [v for v in self]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our environment => like self.env = gym.make('CartPole-v0') from Lab02\n",
        "class SimpleGridWorld(object):\n",
        "\n",
        "  def __init__(self, width: int = 5, height: int = 5, debug: bool = False):\n",
        "    print(\"This is our environment\")\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.debug = debug\n",
        "    self.action_space = [d for d in Direction]\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.cur_pos = Point(x=0, y=(self.height - 1))\n",
        "    self.goal = Point(x=(self.width - 1), y=0)\n",
        "    # If debug, print state\n",
        "    if self.debug:\n",
        "      print(self)\n",
        "    return self.cur_pos, 0, False\n",
        "\n",
        "  def step(self, action: Direction):\n",
        "    # Depending on the action, mutate the environment state\n",
        "    if action == Direction.NORTH:\n",
        "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y + 1)\n",
        "    elif action == Direction.EAST:\n",
        "      self.cur_pos = Point(self.cur_pos.x + 1, self.cur_pos.y)\n",
        "    elif action == Direction.SOUTH:\n",
        "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y - 1)\n",
        "    elif action == Direction.WEST:\n",
        "      self.cur_pos = Point(self.cur_pos.x - 1, self.cur_pos.y)\n",
        "    # Check if out of bounds\n",
        "    if self.cur_pos.x >= self.width:\n",
        "      self.cur_pos = Point(self.width - 1, self.cur_pos.y)\n",
        "    if self.cur_pos.y >= self.height:\n",
        "      self.cur_pos = Point(self.cur_pos.x, self.height - 1)\n",
        "    if self.cur_pos.x < 0:\n",
        "      self.cur_pos = Point(0, self.cur_pos.y)\n",
        "    if self.cur_pos.y < 0:\n",
        "      self.cur_pos = Point(self.cur_pos.x, 0)\n",
        "\n",
        "    # If at goal, terminate\n",
        "    is_terminal = self.cur_pos == self.goal\n",
        "\n",
        "    # Constant -1 reward to promote speed-to-goal\n",
        "\n",
        "    reward = -1\n",
        "\n",
        "    # If debug, print state\n",
        "    if self.debug:\n",
        "      print(self)\n",
        "\n",
        "    return self.cur_pos, reward, is_terminal\n",
        "\n",
        "  def peek(self, action: Direction):\n",
        "  # get next position without mutating the environment\n",
        "    if action == Direction.NORTH:\n",
        "      new_pos = Point(self.cur_pos.x, self.cur_pos.y + 1)\n",
        "    elif action == Direction.EAST:\n",
        "       new_pos = Point(self.cur_pos.x + 1, self.cur_pos.y)\n",
        "    elif action == Direction.SOUTH:\n",
        "       new_pos = Point(self.cur_pos.x, self.cur_pos.y - 1)\n",
        "    elif action == Direction.WEST:\n",
        "      new_pos = Point(self.cur_pos.x - 1, self.cur_pos.y)\n",
        "    # Check if out of bounds\n",
        "    if new_pos.x >= self.width:\n",
        "      new_pos = Point(self.width - 1, self.cur_pos.y)\n",
        "    if new_pos.y >= self.height:\n",
        "      new_pos = Point(self.cur_pos.x, self.height - 1)\n",
        "    if new_pos.x < 0:\n",
        "      new_pos = Point(0, self.cur_pos.y)\n",
        "    if new_pos.y < 0:\n",
        "      new_pos = Point(self.cur_pos.x, 0)\n",
        "    return new_pos\n",
        "\n",
        "  def get_valid_actions(self):\n",
        "    # get only the actions that change the current position\n",
        "    valid_actions = []\n",
        "    for action in self.action_space:\n",
        "      if self.peek(action) != self.cur_pos:\n",
        "        valid_actions.append(action)\n",
        "    return valid_actions\n",
        "\n",
        "  def __repr__(self):\n",
        "    res = \"\"\n",
        "    for y in reversed(range(self.height)):\n",
        "      for x in range(self.width):\n",
        "        if self.goal.x == x and self.goal.y == y:\n",
        "          if self.cur_pos.x == x and self.cur_pos.y == y:\n",
        "            res += \"@\"\n",
        "          else:\n",
        "            res += \"o\"\n",
        "          continue\n",
        "        if self.cur_pos.x == x and self.cur_pos.y == y:\n",
        "          res += \"x\"\n",
        "        else:\n",
        "          res += \"_\"\n",
        "      res += \"\\n\"\n",
        "    return res"
      ],
      "metadata": {
        "id": "POTVXWVc6lGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloGeneration(object):\n",
        "  def __init__(self, env: object, max_steps: int = 1000, debug: bool = False, decay=50, min_epsilon=0.1):\n",
        "    self.env = env\n",
        "    self.max_steps = max_steps\n",
        "    self.debug = debug\n",
        "    self.decay = decay\n",
        "    self.min_epsilon = min_epsilon\n",
        "\n",
        "  def run(self, n_run) -> List:\n",
        "    buffer = []\n",
        "    n_steps = 0 # Keep track of the number of steps so I can bail out if it takes too long\n",
        "    state, _, _ = self.env.reset() # Reset environment back to start\n",
        "    terminal = False\n",
        "    while not terminal: # Run until terminal state\n",
        "\n",
        "        action = self.choose_action(n_run)\n",
        "\n",
        "        next_state, reward, terminal = self.env.step(action) # Take action in environment\n",
        "        buffer.append((state, action, reward)) # Store the result\n",
        "        state = next_state # Ready for the next step\n",
        "        n_steps += 1\n",
        "        if n_steps >= self.max_steps:\n",
        "          if self.debug:\n",
        "            print(\"Terminated early due to large number of steps\")\n",
        "          terminal = True # Bail out if we've been working for too long\n",
        "    return buffer\n",
        "\n",
        "    # Self written methods to implement Task 1 from Lab03\n",
        "\n",
        "  def get_epsilon(self, t):\n",
        "      \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
        "      # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
        "      return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "  def choose_action(self, n_run):\n",
        "    # action_space = all possible actions to perform\n",
        "    action_space = self.env.action_space\n",
        "\n",
        "    epsilon = self.get_epsilon(n_run)  # epsilon = [0;1]\n",
        "    if random.random() < epsilon:\n",
        "      action = random.choice(action_space)\n",
        "    else:\n",
        "      action = max(self.env.action_space, key=lambda a: agent.state_value(self.env.peek(a))) #here choose action which result in the state with the highest value of the state-value function V(s)\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "5qVUZWM37U2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class is our agent\n",
        "class MonteCarloExperiment(object):\n",
        "  def __init__(self, generator: MonteCarloGeneration):\n",
        "    self.generator = generator\n",
        "    self.values = defaultdict(float)\n",
        "    self.counts = defaultdict(float)\n",
        "\n",
        "  def _to_key(self, state):\n",
        "    return (state)\n",
        "\n",
        "  def state_value(self, state) -> float:\n",
        "    key = self._to_key(state)\n",
        "    if self.counts[key] > 0:\n",
        "        return self.values[key] / self.counts[key]\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "\n",
        "  def run_episode(self, n_run) -> None:\n",
        "    trajectory = self.generator.run(n_run) # Generate a trajectory\n",
        "    #print(\"this is trjectory: \", trajectory)\n",
        "    episode_reward = 0\n",
        "    for i, t in enumerate(reversed(trajectory)): # Starting from the terminal state\n",
        "      state, action, reward = t\n",
        "      key = self._to_key(state)\n",
        "      episode_reward += reward  # Add the reward to the buffer\n",
        "      self.values[key] += episode_reward # add the cumulated reward until now (= return = V_n-1) to V_n\n",
        "      self.counts[key] += 1 # Increment counter"
      ],
      "metadata": {
        "id": "h1Ow4xMd8Egr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def state_value_2d(env, agent):\n",
        "    res = \"\"\n",
        "    for y in reversed(range(env.height)):\n",
        "      for x in range(env.width):\n",
        "        if env.goal.x == x and env.goal.y == y:\n",
        "          res += \"   @  \"\n",
        "        else:\n",
        "          state_value = agent.state_value(Point(x,y))\n",
        "          res += f'{state_value:6.2f}'\n",
        "        res += \" | \"\n",
        "      res += \"\\n\"\n",
        "    return res"
      ],
      "metadata": {
        "id": "SUh6Lj1APqcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SimpleGridWorld() # Instantiate the environment\n",
        "generator = MonteCarloGeneration(env=env) # Instantiate the trajectory generator\n",
        "agent = MonteCarloExperiment(generator=generator)\n",
        "for i in range(1000):\n",
        "  clear_output(wait=True)\n",
        "  agent.run_episode(i)\n",
        "  print(f\"Iteration: {i}\")\n",
        "  print(state_value_2d(env, agent))\n",
        "  # time.sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxlsKBYwOdOr",
        "outputId": "415a1dc3-7e0a-41e2-ce4d-33f02ac90951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 999\n",
            "-49.61 | -83.70 | -49.54 | -70.05 | -90.27 | \n",
            "-42.52 | -71.64 | -61.40 | -63.31 | -80.10 | \n",
            "-43.38 | -31.84 | -21.03 | -45.06 | -74.02 | \n",
            "-51.38 | -47.71 | -15.25 | -10.33 | -28.23 | \n",
            "-64.40 | -46.79 | -29.31 |  -4.74 |    @   | \n",
            "\n"
          ]
        }
      ]
    }
  ]
}