{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "# Graded lab: Implement DQN for LunarLander\n",
        "\n",
        "This notebook originates from the Deep RL Course on HuggingFace and has been modified.\n",
        "You're not expected to understand the topic of PPO yet, so you can safely ignore that part.\n",
        "\n",
        "![Cover](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg)\n",
        "\n",
        "In this notebook, you'll train your **DQN agent** - a Lunar Lander agent that will learn to **land correctly on the Moon üåï**. Using [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) a Deep Reinforcement Learning library, share them with the community, and experiment with different configurations\n",
        "\n",
        "‚¨áÔ∏è Here is an example of what **you will achieve in just a couple of minutes.** ‚¨áÔ∏è\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "PF46MwbZD00b",
        "outputId": "fcf7f31f-155a-4518-9b48-cd3dd16257fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7oR6R-ZIbeS"
      },
      "source": [
        "### The environment üéÆ\n",
        "\n",
        "- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "### The library used üìö\n",
        "\n",
        "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwEcFHe9RRZW"
      },
      "source": [
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDAH0h0EBiG"
      },
      "source": [
        "## Install dependencies and create a virtual screen üîΩ\n",
        "\n",
        "The first step is to install the dependencies, we‚Äôll install multiple ones.\n",
        "\n",
        "- `gymnasium[box2d]`: Contains the LunarLander-v2 environment üåõ\n",
        "- `stable-baselines3[extra]`: The deep reinforcement learning library.\n",
        "- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ü§ó Hub.\n",
        "\n",
        "To make things easier, we created a script to install all these dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQIGLPDkGhgG",
        "outputId": "2d4c4c10-26ca-49a1-a561-c2e11304cd0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.1).\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,388 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 120899 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt install swig cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XaULfDZDvrC",
        "outputId": "918ca914-ab21-4d91-c511-caf56ce0b68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3==2.0.0a5 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n",
            "  Downloading stable_baselines3-2.0.0a5-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n",
            "  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium[box2d] (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3))\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_sb3 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4))\n",
            "  Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting gymnasium==0.28.1 (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.1.0+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.7.1)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of gymnasium[box2d] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gymnasium[box2d] (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3))\n",
            "  Downloading gymnasium-0.29.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting box2d-py==2.3.5 (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (0.19.4)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (6.0.1)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349113 sha256=f51033bb1ee7bf84a69be81e622256b051dfeeccf7dff16f06f47dcd58b6427a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, farama-notifications, box2d-py, pygame, jax-jumpy, gymnasium, stable-baselines3, huggingface_sb3\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.28.1 huggingface_sb3-3.0 jax-jumpy-1.0.0 pygame-2.1.3 stable-baselines3-2.0.0a5 swig-4.1.1.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEKeXQJsQCYm"
      },
      "source": [
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install virtual screen libraries and create and run a virtual screen üñ•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5f2cGkdP-mb",
        "outputId": "925bd265-8a7a-4ca9-a756-d0f48c4d6e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 14.2 kB/110 k\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connected to cloud.r-proj\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Waiting for headers] [Wai\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [3 InRelease 1,581 B/1,581 B 100\r                                                                               \r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:4 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,282 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,512 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,036 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [633 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,572 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,576 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,304 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
            "Fetched 9,291 kB in 2s (4,615 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libglu1-mesa\n",
            "Suggested packages:\n",
            "  libgle3 python3-numpy\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libglu1-mesa python3-opengl\n",
            "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 824 kB of archives.\n",
            "After this operation, 8,092 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n",
            "Fetched 824 kB in 1s (1,106 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 121652 files and directories currently installed.)\n",
            "Preparing to unpack .../freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package python3-opengl.\n",
            "Preparing to unpack .../python3-opengl_3.1.5+dfsg-1_all.deb ...\n",
            "Unpacking python3-opengl (3.1.5+dfsg-1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up python3-opengl (3.1.5+dfsg-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 7,814 kB in 1s (7,027 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 124736 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCwBTAwAW9JJ"
      },
      "source": [
        "To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYvkbef7XEMi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE5JWP5rQIKf",
        "outputId": "8c22fb3b-12da-4159-bd97-f4bbb6b368b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x780340159720>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgpVFqyENVf"
      },
      "source": [
        "## Import the packages üì¶\n",
        "\n",
        "One additional library we import is huggingface_hub **to be able to upload and download trained models from the hub**.\n",
        "\n",
        "\n",
        "The Hugging Face Hub ü§ó works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow you to easily collaborate with others.\n",
        "\n",
        "You can see here all the Deep reinforcement Learning models available hereüëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cygWLPGsEQ0m"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIrKGGSlENZB"
      },
      "source": [
        "## Create the LunarLander environment üåõ and understand how it works\n",
        "\n",
        "### [The environment üéÆ](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "In this first tutorial, we‚Äôre going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **to land correctly on the moon**. To do that, the agent needs to learn **to adapt its speed and position (horizontal, vertical, and angular) to land correctly.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "üí° A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNPG0g_UGCfh",
        "outputId": "634913f1-4349-42b2-b95e-8b778828c427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-0.00258131,  1.4116296 , -0.26146847,  0.03152581,  0.00299783,\n",
            "        0.05922648,  0.        ,  0.        ], dtype=float32), {})\n",
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "Observation Space Shape (8,)\n",
            "Sample observation [-47.227703    79.67672      2.363438     2.4818523    0.51080126\n",
            "   4.576614     0.2436623    0.67654735]\n"
          ]
        }
      ],
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "state = env.reset()\n",
        "print(state)\n",
        "\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg contact point has touched the land (boolean)\n",
        "- If the right leg contact point has touched the land (boolean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "We5WqOBGLoSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb1ed54-a685-46a1-9123-662f9a120380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "Action Space Shape 4\n",
            "Action Space Sample 3\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n",
        "\n",
        "- Action 0: Do nothing,\n",
        "- Action 1: Fire left orientation engine,\n",
        "- Action 2: Fire the main engine,\n",
        "- Action 3: Fire right orientation engine.\n",
        "\n",
        "Reward function (the function that will gives a reward at each timestep) üí∞:\n",
        "\n",
        "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
        "-  Is increased/decreased the slower/faster the lander is moving.\n",
        "- Is decreased the more the lander is tilted (angle not horizontal).\n",
        "- Is increased by 10 points for each leg that is in contact with the ground.\n",
        "- Is decreased by 0.03 points each frame a side engine is firing.\n",
        "- Is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
        "\n",
        "An episode is **considered a solution if it scores at least 200 points.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFD9RAFjG8aq"
      },
      "source": [
        "#### Vectorized Environment\n",
        "\n",
        "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99hqQ_etEy1N",
        "outputId": "1c9de969-6eb6-405d-c1ae-59889e2fdcfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "# Create the environment\n",
        "envs = make_vec_env('LunarLander-v2', n_envs=16)\n",
        "print(envs.action_space.sample()) #test random action on envs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrE86r5E5IK"
      },
      "source": [
        "## Create the Model ü§ñ\n",
        "\n",
        "We have studied our environment and we understood the problem: **being able to land the Lunar Lander to the Landing Pad correctly by controlling left, right and main orientation engine**. Now let's build the algorithm we're going to use to solve this Problem üöÄ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV4yiUM_9_Ka"
      },
      "source": [
        "To solve this problem, you're going to implement DQN from scratch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "MfYMekTtBUDO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "      \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "      experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "      states = [torch.from_numpy(np.array(e.state)).float().to(device) for e in experiences if e is not None]\n",
        "      actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "      rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "      next_states = [torch.from_numpy(np.array(e.next_state)).float().to(device) for e in experiences if e is not None]\n",
        "      dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "      return states, actions, rewards, next_states, dones\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "xElz0K0jv80F"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init"
      ],
      "metadata": {
        "id": "jMIUK9Mn_04f"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "nxI6hT1GE4-A"
      },
      "outputs": [],
      "source": [
        "# TODO: Define your DQN agent from scratch!\n",
        "\n",
        "# Here He-Initialization is used for better gradient flow within the network\n",
        "class DQN_Lunar_Lander(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN_Lunar_Lander, self).__init__()\n",
        "\n",
        "        self.layerList = nn.ModuleList()\n",
        "        # n_observations = 8 here with LunarLander-V2 env\n",
        "        self.input_size = n_observations\n",
        "        self.hidden_layer_size = 4\n",
        "        self.num_hidden_layers = 1\n",
        "        # n_actions = 4 here with LunarLander-V2 env\n",
        "        self.output_size = n_actions\n",
        "\n",
        "        # Input Layer\n",
        "        self.layerList.append(nn.Linear(self.input_size, self.hidden_layer_size))\n",
        "        self.layerList.append(nn.ReLU())\n",
        "\n",
        "        # Hidden Layers\n",
        "        for _ in range(self.num_hidden_layers):\n",
        "          linear_layer = nn.Linear(self.hidden_layer_size, self.hidden_layer_size)\n",
        "          init.kaiming_uniform_(linear_layer.weight, mode='fan_in', nonlinearity='relu')\n",
        "          self.layerList.append(linear_layer)\n",
        "          self.layerList.append(nn.ReLU())\n",
        "\n",
        "        # Output Layer\n",
        "        linear_output_layer = nn.Linear(self.hidden_layer_size, self.output_size)\n",
        "        init.kaiming_uniform_(linear_output_layer.weight, mode='fan_in', nonlinearity='linear')\n",
        "        self.layerList.append(linear_output_layer)\n",
        "        # END TODO\n",
        "\n",
        "    def forward(self, state):\n",
        "      for i, layer in enumerate(self.layerList):\n",
        "        state = layer(state)\n",
        "        # print(f\"Output after layer {i + 1}: {x.shape}\")\n",
        "        # print(f\"x in forward: {x}\")\n",
        "      return state\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJJk88yoBUi"
      },
      "source": [
        "## Train the DQN agent üèÉ\n",
        "- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~20min, but you can use fewer timesteps if you just want to try it out.\n",
        "- During the training, take a ‚òï break you deserved it ü§ó"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter definition\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64 # minibatch size for more generalized training\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 1e-3 # for soft update of target parameters\n",
        "LR = 5e-4\n",
        "UPADATE_EVERY = 4"
      ],
      "metadata": {
        "id": "t4hvCn4dFwTI"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent_Lunar_Lander():\n",
        "  def __init__(self, state_size, action_size, seed) -> None:\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "    # Q-Networks with the Adam optimizer, using L2 regularization (weight_decay) for further improvement to find generalized optimum\n",
        "    # action-value network also called: Online Network\n",
        "    self.action_value_net = DQN_Lunar_Lander(state_size, action_size).to(device)\n",
        "    self.target_net = DQN_Lunar_Lander(state_size, action_size).to(device)\n",
        "    self.optimizer = optim.AdamW(self.action_value_net.parameters(), lr=LR, weight_decay=1e-5, amsgrad=True)\n",
        "\n",
        "    # Replay Buffer\n",
        "    # self.replay_buffer = ReplayMemory(BUFFER_SIZE)\n",
        "    self.replay_buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "    self.t_step = 0\n",
        "\n",
        "  def get_epsilon(self, steps):\n",
        "    \"\"\"Gets value for epsilon. It declines as we take more steps.\"\"\"\n",
        "    # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
        "    return EPS_START * max(EPS_END, min(1., 1. - math.log10((steps + 1) / EPS_DECAY)))\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "    if self.t_step == 0:\n",
        "      if len(self.replay_buffer) > BATCH_SIZE:\n",
        "        experiences = self.replay_buffer.sample()\n",
        "        self.learn(experiences, GAMMA)\n",
        "\n",
        "  def act(self, states):\n",
        "    states = torch.cat(states, dim=0)  # Concatenate the states into a single tensor\n",
        "    sample = random.random()\n",
        "    actions = []\n",
        "    for state in states:\n",
        "      eps_threshold = self.get_epsilon(self.t_step)\n",
        "      if sample > eps_threshold:\n",
        "          with torch.no_grad():\n",
        "              actions.append(self.action_value_net.forward(state).max(1)[1].item())\n",
        "      else:\n",
        "          actions.append(envs.action_space.sample())\n",
        "      self.t_step += 1\n",
        "    return actions\n",
        "\n",
        "\n",
        "  def learn(self, experiences, gamma):\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "    # implementation of the slide p. 17 / lecture 11\n",
        "    q_targets_next = self.target_net(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    target_q_values = rewards + gamma * q_targets_next * (1 - dones)\n",
        "    q_expected = self.action_value_net(states).gather(1, actions)\n",
        "\n",
        "    # because of DQN => MSE as loss-fucntion\n",
        "    loss = F.mse_loss(q_expected, target_q_values)\n",
        "    self.optimizer.zero_grads()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # update of the\n",
        "    self.soft_update(self.action_value_net, self.target_net, TAU)\n",
        "\n",
        "\n",
        "  def soft_update(self, local_model, target_model, tau):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "        target_param.data.copy_(tau * local_param.data + (1.0-tau) * target_param.data)"
      ],
      "metadata": {
        "id": "_ey7kXvGGx-Z"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the state_size for the DQNs\n",
        "state_sample = envs.reset()\n",
        "state_size = len(state_sample[0])\n",
        "# get the action_size for the DQNs\n",
        "action_size = envs.action_space.n\n",
        "\n",
        "agent = Agent_Lunar_Lander(state_size, action_size, seed=42)"
      ],
      "metadata": {
        "id": "sduzFdM8j_-t"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn(envs, agent, n_episodes=2000, max_t=1000):\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        states = envs.reset()  # Reset all environments\n",
        "        states = [torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) for state in states]\n",
        "        rewards_accumulator = np.zeros(envs.num_envs)\n",
        "\n",
        "        for t in range(max_t):\n",
        "            # print(states)\n",
        "            actions = agent.act(states)\n",
        "            next_state, reward, done, _ = envs.step(actions)\n",
        "\n",
        "            for i in range(envs.num_envs):\n",
        "                agent.step(states[i], actions[i], reward[i], next_state[i], done[i])\n",
        "                rewards_accumulator[i] += reward[i]\n",
        "\n",
        "            # states = [torch.tensor(next_state[i], dtype=torch.float32, device=device).unsqueeze(0) for i in range(envs.num_envs)]  # Update states\n",
        "            # states = [torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0) for state in states]\n",
        "            states = [torch.tensor(state, dtype=torch.float32).clone().detach().to(device).unsqueeze(0) for state in states]\n",
        "            if np.any(done):\n",
        "                break\n",
        "\n",
        "        avg_score = np.mean(rewards_accumulator)\n",
        "        scores_window.append(avg_score)\n",
        "        scores.append(avg_score)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        if np.mean(scores_window) >= 200.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "C4kEt5JsofHR"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn(envs, agent)"
      ],
      "metadata": {
        "id": "rLiO6oiLoiw2",
        "outputId": "65dd1726-7a01-4528-c815-bc37f1fe8c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-136-a4a710041bec>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = [torch.tensor(state, dtype=torch.float32).clone().detach().to(device).unsqueeze(0) for state in states]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-1fa042a6cc69>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-136-a4a710041bec>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(envs, agent, n_episodes, max_t)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# print(states)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-134-16ae2ef1a0ee>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0meps_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m               \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_value_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 4 elements cannot be converted to Scalar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn_training(n_episodes=1, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "  scores = []                        # list containing scores from each episode\n",
        "  scores_window = deque(maxlen=100)  # last 100 scores\n",
        "  eps = eps_start                    # initialize epsilon\n",
        "  episode_durations = [[] for _ in range(envs.num_envs)]\n",
        "  for i_episode in range(1, n_episodes+1):\n",
        "      state = envs.reset()\n",
        "      states = [torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) for state in states]\n",
        "      rewards_accumulator = np.zeros(envs.num_envs)\n",
        "      # score = 0\n",
        "      for t in range(max_t):\n",
        "          actions = agent.act(states, eps)\n",
        "          print(\"actions=\", actions)\n",
        "          next_states, rewards, dones, _ = env.step(actions)\n",
        "          observations = [torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0) for observation in next_states]\n",
        "          rewards = [torch.tensor([reward], device=device) for reward in rewards]\n",
        "          actions = [torch.tensor([[action]], device=device) for action in actions]\n",
        "          for env_index in range(envs.num_envs):\n",
        "            rewards_accumulator[env_index] += rewards[env_index].item()\n",
        "            if dones[env_index]:\n",
        "                next_state = None\n",
        "                episode_durations[env_index].append(rewards_accumulator[env_index])\n",
        "                # plot_durations()\n",
        "                rewards_accumulator[env_index] = 0\n",
        "            else:\n",
        "                next_state = observations[env_index]\n",
        "\n",
        "          agent.step(state, action, reward, next_state, done)\n",
        "          state = next_state\n",
        "          # score += reward\n",
        "          rewards_accumulator[env_index] += rewards[env_index].item()\n",
        "          if done:\n",
        "              break\n",
        "      scores_window.append(score)       # save most recent score\n",
        "      scores.append(score)              # save most recent score\n",
        "      eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "      if i_episode % 100 == 0:\n",
        "          print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "      if np.mean(scores_window)>=200.0:\n",
        "          print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "          torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "    return scores"
      ],
      "metadata": {
        "id": "yK6FdzBVcctu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def select_actions(states):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    actions = []\n",
        "    for state in states:\n",
        "      eps_threshold = get_epsilon(steps_done)\n",
        "      if sample > eps_threshold:\n",
        "          with torch.no_grad():\n",
        "              actions.append(policy_net.forward(state).max(1)[1].item())\n",
        "      else:\n",
        "          actions.append(envs.action_space.sample())\n",
        "      steps_done += 1\n",
        "    return actions\n",
        "\n",
        "\n",
        "episode_durations = [[] for _ in range(envs.num_envs)]\n",
        "\n",
        "def get_epsilon(steps):\n",
        "  \"\"\"Gets value for epsilon. It declines as we take more steps.\"\"\"\n",
        "  # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
        "  return EPS_START * max(EPS_END, min(1., 1. - math.log10((steps + 1) / EPS_DECAY)))\n",
        "\n",
        "\n",
        "def plot_durations(show_result=False):\n",
        "\n",
        "    # Plot durations for each environment\n",
        "    for env_index in range(envs.num_envs):\n",
        "        plt.figure(env_index + 1)  # Start a new figure for each environment\n",
        "        durations_t = torch.tensor(episode_durations[env_index], dtype=torch.float)\n",
        "        if show_result:\n",
        "            plt.title(f'Environment {env_index + 1} - Result')\n",
        "        else:\n",
        "            plt.clf()\n",
        "            plt.title(f'Environment {env_index + 1} - Training...')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Duration')\n",
        "        plt.plot(durations_t.numpy())\n",
        "        # Take 100 episode averages and plot them too\n",
        "        if len(durations_t) >= 100:\n",
        "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "            means = torch.cat((torch.zeros(99), means))\n",
        "            plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ],
      "metadata": {
        "id": "ZpfflIaeB8HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "    # Compute the expected state-action values (Q values)\n",
        "    # TODO: write the function to compute the expected Q values\n",
        "    #   Q(s,a) = Q(s,a)*GAMMA +  r\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "mbAyW7j2CPUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQzQ-QcE3zo"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poBCy9u_csyR"
      },
      "outputs": [],
      "source": [
        "# Initialize the environment and get it's state. environments resets automatically when reaching a terminal state\n",
        "states = envs.reset() # 2d array containing the 16 observations of the 16 environments\n",
        "# print(states.shape)\n",
        "# print(states)\n",
        "states = [torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) for state in states]\n",
        "# print(states.shape)\n",
        "# print(states)\n",
        "\n",
        "rewards_accumulator = np.zeros(envs.num_envs)\n",
        "\n",
        "# TODO: Train it for 1,000,000 timesteps\n",
        "while steps_done < 1000000:\n",
        "    actions = select_actions(states)\n",
        "    # print(\"action\", actions)\n",
        "    observations, rewards, dones, _ = envs.step(actions)\n",
        "    observations = [torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0) for observation in observations]\n",
        "    # print(\"rewards\", rewards)\n",
        "    rewards = [torch.tensor([reward], device=device) for reward in rewards]\n",
        "    actions = [torch.tensor([[action]], device=device) for action in actions]\n",
        "    # print(\"element of actions\", actions[0])\n",
        "    for env_index in range(envs.num_envs):\n",
        "        rewards_accumulator[env_index] += rewards[env_index].item()\n",
        "        if dones[env_index]:\n",
        "            next_state = None\n",
        "            episode_durations[env_index].append(rewards_accumulator[env_index])\n",
        "            # plot_durations()\n",
        "            rewards_accumulator[env_index] = 0\n",
        "        else:\n",
        "            next_state = observations[env_index]\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(states[env_index], actions[env_index], next_state, rewards[env_index])\n",
        "\n",
        "    # Move to the next state\n",
        "    states = observations\n",
        "\n",
        "    # Perform one step of the optimization (on the policy network)\n",
        "    optimize_model()\n",
        "\n",
        "    # Soft update of the target network's weights\n",
        "    # Œ∏‚Ä≤ ‚Üê œÑ Œ∏ + (1 ‚àíœÑ )Œ∏‚Ä≤\n",
        "    target_net_state_dict = target_net.state_dict()\n",
        "    policy_net_state_dict = policy_net.state_dict()\n",
        "    for key in policy_net_state_dict:\n",
        "        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "    target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_HuedOoISR"
      },
      "source": [
        "## Evaluate the agent üìà\n",
        "- Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).\n",
        "- Now that our Lunar Lander agent is trained üöÄ, we need to **check its performance**.\n",
        "\n",
        "\n",
        "üí° When you evaluate your agent, you should not use your training environment but create an evaluation environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reBhoODwcXfr"
      },
      "source": [
        "- In my case, I got a mean reward is `200.20 +/- 20.80` after training for 1 million steps, which means that our lunar lander agent is ready to land on the moon üåõü•≥."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "from gym.wrappers import RecordVideo\n",
        "\n",
        "# For Google Colab, to download files\n",
        "from google.colab import files\n",
        "\n",
        "# Create a directory to store video\n",
        "video_folder = '/content/videos'\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Wrap your environment\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env = RecordVideo(env, video_folder)\n",
        "\n",
        "def evaluate_and_record(policy_net, env, n_eval_episodes=1):\n",
        "    for i in range(n_eval_episodes):\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            state_tensor = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = policy_net(state_tensor).max(1)[1].view(1, 1).item()\n",
        "            state, _, done, _ = env.step(action)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    # Close the environment and video recorder\n",
        "    env.close()\n",
        "\n",
        "# Evaluate and record\n",
        "evaluate_and_record(policy_net, env)\n",
        "\n",
        "# Download the video files\n",
        "for filename in os.listdir(video_folder):\n",
        "    if filename.endswith(\".mp4\"):\n",
        "        files.download(os.path.join(video_folder, filename))\n"
      ],
      "metadata": {
        "id": "_t9pmgJsHuRo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}